{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3pCHb-BiJ1R"
   },
   "source": [
    "# Lab 7: Sentiment analysis with an LSTM network\n",
    "## Grade: 3/3\n",
    "This week, we'll build a model for sentiment analysis, the problem of taking a string of text and predicting how positive an opinion it expresses.\"\n",
    "To do this, we'll use the last two big ideas in the course: vector embeddings and recurrent neural networks (with LSTM cells), trained on a dataset of [IMDB movie reviews](http://ai.stanford.edu/~amaas/data/sentiment/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1i9vagsuiLnG"
   },
   "source": [
    "# Link: https://colab.research.google.com/drive/1ckDx2BBwcF2yEJIH8xOmtuK5sK6mK-59#scrollTo=1i9vagsuiLnG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "0Y5MPlAeiJ1W"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2I9mKIeBiJ1Y"
   },
   "source": [
    "## Section 0: Preprocess and understand the data\n",
    "This dataset is built into Keras, so it's very easy to import.\n",
    "I've written the preprocessing pipeline, but make sure to read it -- it'll be essential for understanding the data you're building a model for. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdLsD0rFiJ1Z"
   },
   "source": [
    "### 0.1: Load the data\n",
    "There are two hyperparameters here:\n",
    " - `maxlen`: The maximum number of words per review. Reviews longer than this are truncated. Keeping this low makes training faster by reducing the number of steps needed per example, but in practice we'd probably increase it. \n",
    " - `num_words`: The number of distinct words the dataset will contain. The `num_words` most common words are assigned unique tokens, and the rest are grouped into a single token.\n",
    " \n",
    "If training is taking forever, feel free to reduce `maxlen`.\n",
    "You can also try changing `num_words` to investigate the tradeoff it induces between the statistical and computational efficiency of having fewer unique words by grouping uncommon words and the advantages of recognizing more words.\n",
    "\n",
    "Reviews are returned as a sequence of integer tokens, each of which represents a distinct word.\n",
    "There are 3 special tokens:\n",
    " - 0 is a padding token (see below)\n",
    " - 1 is a token that represents the start of a review\n",
    " - 2 is a token that represents a word not in the model's vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "zWe64iCpiJ23"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "# Hyperparameters\n",
    "maxlen = 256\n",
    "num_words = 5000\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = \\\n",
    "    imdb.load_data(maxlen=maxlen, num_words=num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nT7SPEfbiJ27"
   },
   "source": [
    "### 0.2: Pad all reviews to the same length\n",
    "Training is much more efficient when we can stack an entire batch of reviews together in a single tensor, so Keras requires that every training sequence is of the same length.\n",
    "To do this, we add padding tokens (the 0 token) to the beginning of every sequence to make them all of length `maxlen`.\n",
    "\n",
    "We pad the beginning of the sequence instead because padding the end would cause many steps of the RNN after it's read the last word in the review, causing the hidden state to lose information.\n",
    "In the model, we'll also tell the recurrent layers to mask out 0 values, so that the hidden state of the network is the same every time it reaches the start token (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "KlR5auaIiJ2-"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_r5-Qai7iJ2_"
   },
   "source": [
    "### 0.3: Build word-token dictionaries\n",
    "In order to use the model with text outside of the dataset, we need to be able to convert words into tokens.\n",
    "We build two dictionaries:\n",
    " - `word_index` maps words into tokens\n",
    " - `index_word` maps tokens into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "mZXo3AZ6iJ3A"
   },
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "index_word = {k + 3: v for (v, k) in word_index.items()}\n",
    "index_word[0] = '<PAD>'   # Special padding token\n",
    "index_word[1] = '<START>' # Special \"start of review\" token\n",
    "index_word[2] = '<OOV>'   # Special \"out of vocabulary\" token \n",
    "word_index = {k: v for (v, k) in index_word.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYl2uqOkiJ3C"
   },
   "source": [
    "### 0.4: Using the dataset\n",
    "Below we print some summary statistics of the dataset and show how to convert between text and tokenized form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iGziDQF5iJ3E",
    "outputId": "5c09563f-01a7-4aa9-baa5-d43be7c53966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set size: (17416, 256)\n",
      "Test set size: (17843, 256)\n",
      "Number of tokens: 88587\n",
      "Vocabulary size: 5000\n",
      "Proportion of words that are out-of-vocabulary: 0.3906%\n",
      "\n",
      "\n",
      "Review converted from tokens:\n",
      " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> big hair big <OOV> bad music and a giant safety <OOV> these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an <OOV> the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are <OOV> and funny in equal <OOV> the hair is big lots of <OOV> <OOV> men wear those cut <OOV> <OOV> that show off their <OOV> <OOV> that men actually wore them and the music is just <OOV> trash that plays over and over again in almost every scene there is trashy music <OOV> and <OOV> taking away bodies and the <OOV> still doesn't close for <OOV> all <OOV> aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\n",
      "\n",
      "Review sentiment: 0\n"
     ]
    }
   ],
   "source": [
    "# Print summary statistics\n",
    "print(\n",
    "'''\n",
    "Training set size: {:}\n",
    "Test set size: {:}\n",
    "Number of tokens: {:}\n",
    "Vocabulary size: {:}\n",
    "Proportion of words that are out-of-vocabulary: {:.4f}%\\n\n",
    "'''.format(x_train.shape, \n",
    "           x_test.shape, \n",
    "           len(index_word.keys()),\n",
    "           num_words,\n",
    "           np.mean(x_train == 1) * 100)\n",
    ")\n",
    "\n",
    "review_idx = 1\n",
    "review_tokens = x_train[review_idx]\n",
    "review_words = [index_word[idx] for idx in review_tokens]\n",
    "print('Review converted from tokens:\\n', ' '.join(review_words))\n",
    "print('\\nReview sentiment:', y_train[review_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEBRfN4xiJ3E"
   },
   "source": [
    "## Section 1: Build a model\n",
    "### Grade: 3/3\n",
    "The data and task really inform how we'll build the model here:\n",
    " - The input is variable-length sequences, so the feature extraction will be recurrent.\n",
    " - Each element of the input sequence is a word token, so the input is sparse and categorical. We'll deal with this by first computing embeddings.\n",
    " - The output is binary classification, so our model should produce a single probability independent of the length of the input sequence.\n",
    " \n",
    "Since this model has a lot of components, including recurrent layers, we'll stick to building the model completely in Keras.\n",
    "I used the functional API but the sequential API would also work here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WR7SOIl1iJ3F"
   },
   "source": [
    "### 1.1: Input and embedding layers\n",
    "Make an embedding layer that takes input of the correct shape and yields word embeddings.\n",
    "\n",
    "Notes:\n",
    " - `mask_zero` should be set to True, which will mask off the padding tokens we added before.\n",
    " - I used 64-dimensional embeddings.\n",
    " - Each input in a batch is a sequence of scalars (integer tokens) of length `maxlen`.\n",
    " - If you want to pass variable-length sequences as input, use None as the dimension on the sequence length axis of the input and don't specify an `input_length` for the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "HIEVMLmFktoy"
   },
   "outputs": [],
   "source": [
    "# NOTE: 3/3\n",
    "\n",
    "from keras.models import Functional\n",
    "from keras.layers import Input, Embedding\n",
    "\n",
    "inputs = Input(shape=(None,))\n",
    "\n",
    "# Get embedding layer separate from its output so we can extract weights\n",
    "embedding_layer = Embedding(input_dim=num_words, output_dim=64, mask_zero=True)\n",
    "embedding_outputs = embedding_layer(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTLMMR-qiJ3k"
   },
   "source": [
    "### 1.2: Recurrent feature-extraction layer\n",
    "Make an LSTM layer to summarize the variable-length sequence of embedding vectors into a fixed-size feature vector.\n",
    "\n",
    "Notes:\n",
    " - We're only interested in the last output of the LSTM layer.\n",
    " - I used 64 units.\n",
    " - You can add more layers if you like to make a deep LSTM network. If you do, the earlier layers should use `return_sequences` to yield an entire sequence of output vectors instead of just the last output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "KLakOEUgiJ3k"
   },
   "outputs": [],
   "source": [
    "# NOTE: 3/3, cool that you went for a deep LSTM!\n",
    "\n",
    "from keras.layers import LSTM\n",
    "\n",
    "LSTM_layer1 = LSTM(units=32, return_sequences=True)\n",
    "LSTM_layer2 = LSTM(units=64)\n",
    "LSTM_outputs = LSTM_layer2(LSTM_layer1(embedding_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSzrXqtNiJ3l"
   },
   "source": [
    "### 1.3: Output layer\n",
    "Add a dense layer to perform the final classification from the summary vector output by the LSTM layer to the probability that the input sequence expresses positive sentiment.\n",
    "\n",
    "Note that this is binary classification, so choose the layer's output size and activation function appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "mHWhLOxIiJ3l"
   },
   "outputs": [],
   "source": [
    "# NOTE: 3/3\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "dense_layer = Dense(units=1, activation='sigmoid')\n",
    "dense_outputs = dense_layer(LSTM_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iI9ap1XLiJ3l"
   },
   "source": [
    "### 1.4: Compile and train model\n",
    "Compile and train the model.\n",
    "\n",
    "Notes:\n",
    " - RMSProp is usually a good choice for optimizing RNNs.\n",
    " - I used `clipnorm=1` in my optimizer to prevent exploding gradients.\n",
    " - I got about 90% accuracy after a couple of training epochs.\n",
    " - RNN training can take a while. Try training for a small number of epochs, or reducing `maxlen` if it takes too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MVatuSfxiJ3k",
    "outputId": "b3974f73-29dc-447c-c571-76e7ee322833"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_20 (InputLayer)       [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_15 (Embedding)    (None, None, 64)          320000    \n",
      "                                                                 \n",
      " lstm_35 (LSTM)              (None, None, 32)          12416     \n",
      "                                                                 \n",
      " lstm_36 (LSTM)              (None, 64)                24832     \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 357,313\n",
      "Trainable params: 357,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# NOTE: 3/3, I wonder if it would increase appreciably if you trained for more epochs\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "optimizer = tf.optimizers.RMSprop(2e-2, clipnorm=1)\n",
    "\n",
    "model = Functional(inputs, dense_outputs)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0MQSCTK0phiB",
    "outputId": "153ce982-0a9f-4624-ad37-8b1c69d735b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "545/545 [==============================] - 274s 485ms/step - loss: 0.4461 - accuracy: 0.7791 - val_loss: 0.3034 - val_accuracy: 0.8782\n",
      "Epoch 2/3\n",
      "545/545 [==============================] - 264s 485ms/step - loss: 0.2674 - accuracy: 0.8976 - val_loss: 0.3871 - val_accuracy: 0.8678\n",
      "Epoch 3/3\n",
      "545/545 [==============================] - 267s 490ms/step - loss: 0.2444 - accuracy: 0.9078 - val_loss: 0.2905 - val_accuracy: 0.8839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_35_layer_call_fn, lstm_cell_35_layer_call_and_return_conditional_losses, lstm_cell_36_layer_call_fn, lstm_cell_36_layer_call_and_return_conditional_losses, lstm_cell_35_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./logs/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./logs/model/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7ff97386eb50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7ff97387e890> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_test,y_test),epochs=3)\n",
    "model.save('./logs/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdbrVsgxiJ3m"
   },
   "source": [
    "## Section 2: Evaluate the model\n",
    "### Grade: 3/3\n",
    "Below, I've pasted a review from IMDB and tokenized it.\n",
    "Add code to run your model over the review to predict whether it expresses positive or negative sentiment.\n",
    "\n",
    "Hint: Your model should output a single probability here, but expects a batch.\n",
    "You might need to use `np.expand_dims()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "x24hA0RdiJ3n"
   },
   "outputs": [],
   "source": [
    "review = \\\n",
    "'''\n",
    "Pulp Fiction may be the single best film ever made, and quite appropriately\n",
    " it is by one of the most creative directors of all time, Quentin Tarantino.\n",
    " This movie is amazing from the beginning definition of pulp to the end \n",
    " credits and boasts one of the best casts ever assembled with the likes of\n",
    " Bruce Willis, Samuel L. Jackson, John Travolta, Uma Thurman, Harvey Keitel,\n",
    " Tim Roth and Christopher Walken. The dialog is surprisingly humorous for\n",
    " this type of film, and I think that\\'s what has made it so successful.\n",
    " Wrongfully denied the many Oscars it was nominated for, Pulp Fiction is by\n",
    " far the best film of the 90s and no Tarantino film has surpassed the \n",
    " quality of this movie (although Kill Bill came close). As far as I\\'m \n",
    " concerned this is the top film of all-time and definitely deserves a \n",
    " watch if you haven\\'t seen it.\n",
    "'''\n",
    "review = ''.join(list(filter(lambda x: x not in '\\',.()\\n', review.lower())))\n",
    "\n",
    "review_tokens = [1] # Begin with the <START> token\n",
    "for word in review.split():\n",
    "    review_tokens.append(word_index[word] if word in word_index.keys() and word_index[word] <= num_words else 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3xXzIVCiiJ3n",
    "outputId": "e4a8a1db-fc00-426f-c376-0e6fb5c368d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99591446]]\n"
     ]
    }
   ],
   "source": [
    "# NOTE: 3/3, good job!\n",
    "\n",
    "print(model.predict(tf.expand_dims(review_tokens,axis=0)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "lab_7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
