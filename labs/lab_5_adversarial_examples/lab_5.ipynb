{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "lab_5.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_Xby4P8rhIN"
      },
      "source": [
        "# Lab 5: A Black-Box Adversarial Attack\n",
        "In this lab, we'll carry out an adversarial attack in the **black box** case: we don't have the weights of the network we're trying to trick (the **target model**), and instead we can only give it inputs and see its outputs.\n",
        "\n",
        "Black-box adversarial attacks are based on the fact that adversarial examples are _transferable_: if it tricks one network, it will likely trick another (but less well).\n",
        "So, since we need network weights to make adversarial examples, we train a second model, the **surrogate model**, to act like the target model.\n",
        "To do this, we'll use the target model as an \"oracle\" to make a dataset where the features are from real examples, and the labels are the outputs of the target model on those examples.\n",
        "If the surrogate model learns to output the same probabilities as the target model on a given input, its internal representations are likely similar.\n",
        "Then, when we make an adversarial example for the surrogate model, it should also trick the target model.\n",
        "\n",
        "The end result of this lab should be an image which looks close to some image in the dataset, but tricks the target network into assigning high confidence to some other class. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV9Cunbcro9m"
      },
      "source": [
        "# Link : https://colab.research.google.com/drive/1Q9A83J4NeXrPjdknJyYScverxizH-P_v#scrollTo=vV9Cunbcro9m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPcFVQG6rhIT"
      },
      "source": [
        "## Section 0: Load the data\n",
        "This uses the same dataset as last time, so just copy the data over here in the same format.\n",
        "The same data loading and preprocessing as before is in place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GKHWzDsrhIU"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "2_895RtLsEh6",
        "outputId": "e197ff34-46b8-48c5-a140-a5a315112169"
      },
      "source": [
        "# I used this to upload the data files onto the google colab platform\n",
        "I also had to manually create a data folder\n",
        "\n",
        "from google.colab import files\n",
        "def getLocalFiles():\n",
        "    _files = files.upload()\n",
        "    if len(_files) >0:\n",
        "       for k,v in _files.items():\n",
        "         open(k,'wb').write(v)\n",
        "getLocalFiles()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d209b200-35b0-4a90-a364-8ba95368b091\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d209b200-35b0-4a90-a364-8ba95368b091\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving X.npy to X.npy\n",
            "Saving Y.npy to Y.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXAByrJMrhIV"
      },
      "source": [
        "# Seed numpy rng for reproducibility\n",
        "np.random.seed(1337)\n",
        "\n",
        "# Load data\n",
        "x_all = np.load('data/X.npy')\n",
        "y_all = np.load('data/Y.npy')\n",
        "\n",
        "# Maps dataset-provided label to true label\n",
        "label_map = {0:9, 1:0, 2:7, 3:6, 4:1, 5:8, 6:4, 7:3, 8:2, 9:5}\n",
        "\n",
        "# Correct dataset labels\n",
        "for row in range(y_all.shape[0]):\n",
        "    dataset_label = np.where(y_all[row])[0][0]\n",
        "    y_all[row, :] = np.zeros(10)\n",
        "    y_all[row, label_map[dataset_label]] = 1\n",
        "    \n",
        "# Shuffle features and targets together\n",
        "# Credit for this technique to:\n",
        "# https://stackoverflow.com/questions/4601373/\n",
        "# better-way-to-shuffle-two-numpy-arrays-in-unison\n",
        "rng_state = np.random.get_state()\n",
        "np.random.shuffle(x_all)\n",
        "np.random.set_state(rng_state)\n",
        "np.random.shuffle(y_all)\n",
        "\n",
        "# Add a dummy channel axis to input images\n",
        "x_all = np.expand_dims(x_all, axis=-1)\n",
        "\n",
        "# Center and rescale data to the range [-1, 1]\n",
        "x_all = x_all - 0.5\n",
        "x_all = x_all * 2\n",
        "\n",
        "# Create a validation set from 30% of the available data\n",
        "n_points = x_all.shape[0]\n",
        "n_test = int(n_points * 0.3)\n",
        "n_train = n_points - n_test\n",
        "x_train, x_test = np.split(x_all, [n_train], axis=0)\n",
        "y_train, y_test = np.split(y_all, [n_train], axis=0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ibdc-ZH1rhIW"
      },
      "source": [
        "## Section 1: Train the black-box model\n",
        "Below, I've written a CNN in Keras to classify images from the dataset, and the code to train it.\n",
        "This will act as the \"black box model.\"\n",
        "\n",
        "Train the model using the code below.\n",
        "It should hit about 95-96% validation accuracy on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA8QPq4IrhIX",
        "outputId": "c9ad5a93-ead8-409d-e7ce-594efaffdf9f"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, Dense, Flatten, MaxPool2D\n",
        "\n",
        "inputs = Input(shape=(64, 64, 1))\n",
        "layer = Conv2D(16, 5, strides=(2, 2), activation='relu')(inputs)\n",
        "layer = Conv2D(16, 3, activation='relu')(layer)\n",
        "layer = MaxPool2D()(layer)\n",
        "\n",
        "layer = Conv2D(32, 3, activation='relu')(layer)\n",
        "layer = Conv2D(32, 3, activation='relu')(layer)\n",
        "layer = MaxPool2D()(layer)\n",
        "\n",
        "layer = Conv2D(64, 3, padding='same', activation='relu')(layer)\n",
        "layer = Conv2D(64, 3, padding='same', activation='relu')(layer)\n",
        "layer = MaxPool2D()(layer)\n",
        "\n",
        "layer = Flatten()(layer)\n",
        "layer = Dense(128, activation='relu')(layer)\n",
        "probs = Dense(10, activation='softmax')(layer)\n",
        "\n",
        "target_model = Model(inputs, probs)\n",
        "target_model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 64, 64, 1)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 30, 30, 16)        416       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 28, 28, 16)        2320      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 16)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 12, 12, 32)        4640      \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 10, 10, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 32)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 5, 5, 64)          18496     \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 5, 5, 64)          36928     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 2, 2, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 106,234\n",
            "Trainable params: 106,234\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ruTgB7drhIY"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "opt = Adam(1e-3)\n",
        "target_model.compile(opt, loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4hO0-a5rhIZ",
        "outputId": "17a52cf4-f249-4643-c105-6d3461466b7e"
      },
      "source": [
        "target_model.fit(x_train, y_train, \n",
        "                 validation_data=(x_test, y_test),\n",
        "                 epochs=15)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "46/46 [==============================] - 5s 77ms/step - loss: 2.1397 - accuracy: 0.2161 - val_loss: 1.3952 - val_accuracy: 0.5518\n",
            "Epoch 2/15\n",
            "46/46 [==============================] - 3s 70ms/step - loss: 0.9751 - accuracy: 0.6891 - val_loss: 0.6179 - val_accuracy: 0.7686\n",
            "Epoch 3/15\n",
            "46/46 [==============================] - 3s 70ms/step - loss: 0.4887 - accuracy: 0.8421 - val_loss: 0.4972 - val_accuracy: 0.8301\n",
            "Epoch 4/15\n",
            "46/46 [==============================] - 3s 70ms/step - loss: 0.3227 - accuracy: 0.8843 - val_loss: 0.4105 - val_accuracy: 0.8657\n",
            "Epoch 5/15\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 0.2073 - accuracy: 0.9321 - val_loss: 0.2722 - val_accuracy: 0.9239\n",
            "Epoch 6/15\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 0.1201 - accuracy: 0.9647 - val_loss: 0.2821 - val_accuracy: 0.9239\n",
            "Epoch 7/15\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 0.0933 - accuracy: 0.9668 - val_loss: 0.2542 - val_accuracy: 0.9223\n",
            "Epoch 8/15\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 0.0443 - accuracy: 0.9868 - val_loss: 0.2472 - val_accuracy: 0.9304\n",
            "Epoch 9/15\n",
            "46/46 [==============================] - 3s 70ms/step - loss: 0.0446 - accuracy: 0.9861 - val_loss: 0.3161 - val_accuracy: 0.9094\n",
            "Epoch 10/15\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 0.0600 - accuracy: 0.9785 - val_loss: 0.3025 - val_accuracy: 0.9142\n",
            "Epoch 11/15\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 0.0763 - accuracy: 0.9744 - val_loss: 0.2089 - val_accuracy: 0.9482\n",
            "Epoch 12/15\n",
            "46/46 [==============================] - 3s 70ms/step - loss: 0.0291 - accuracy: 0.9910 - val_loss: 0.1718 - val_accuracy: 0.9498\n",
            "Epoch 13/15\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 0.0684 - accuracy: 0.9792 - val_loss: 0.2294 - val_accuracy: 0.9320\n",
            "Epoch 14/15\n",
            "46/46 [==============================] - 3s 70ms/step - loss: 0.0310 - accuracy: 0.9889 - val_loss: 0.2459 - val_accuracy: 0.9482\n",
            "Epoch 15/15\n",
            "46/46 [==============================] - 3s 70ms/step - loss: 0.0129 - accuracy: 0.9938 - val_loss: 0.2141 - val_accuracy: 0.9531\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f28576bac90>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFh0m-MXrhIZ"
      },
      "source": [
        "## Section 2: Build an \"oracle dataset\" and data pipeline\n",
        "Now, we'll treat the target model as a black box: pretend we don't have access to its weights or its original input data.\n",
        "In the wild, all we can do is give it inputs and see its outputs.\n",
        "\n",
        "We want to train a surrogate model to act similarly to the target model, so create a fake \"oracle dataset\" where the features are `x_all` and the outputs are the 10-vectors of probability the target model predicts for that input.\n",
        "\n",
        "Then, set up any `tf.data.Dataset` objects you need.\n",
        "In this case we don't have a test set, just a training set. Add `tf.summary.SummaryWriter`s to save logs to `./logs/surrogate` and `./logs/adversarial`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdUI9U6rrhIZ"
      },
      "source": [
        "n_epochs = 3\n",
        "batch_size = 32\n",
        "\n",
        "n_batches_per_epoch = len(y_all) // batch_size\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_all, y_all))\\\n",
        "    .shuffle(x_all.shape[0]).batch(batch_size).cache().repeat(n_epochs)\n",
        "\n",
        "surrogate_writer = tf.summary.create_file_writer('./logs/surrogate')\n",
        "adversarial_writer = tf.summary.create_file_writer('./logs/adversarial')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzUR-mwMrhIa"
      },
      "source": [
        "## Section 3: Build a surrogate model\n",
        "The surrogate model is designed to act similarly to the target model, so that adversarial examples we create for it will also work on the target model.\n",
        "Feel free to use any architecture you want, but matching the target architecture closely (maybe with a bit more capacity) is a good bet.\n",
        "\n",
        "Copy code from last week's assignment liberally.\n",
        "Most likely, you won't have to write all that much new code in this section.\n",
        "\n",
        "This should involve tensors that:\n",
        " - Act as hidden convolutional and dense layers\n",
        " - Compute the logits and probabilities for a batch of input images\n",
        " - Compute the predicted digit from the probabilities\n",
        " - Compute the mean cross-entropy loss over a batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZRZy_AOrhIa"
      },
      "source": [
        "### 3.1 Dense and convolutional layers\n",
        "Copy the `Dense` and `Conv` classes you wrote last week here. You will need them to construct the graph of your surrogate model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC_BFLMMrhIa"
      },
      "source": [
        "class Dense(tf.Module):\n",
        "    '''\n",
        "    Creates a dense layer module.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    dim_input: int\n",
        "        Number of features in the input representation.\n",
        "    dim_output: int\n",
        "        Number of features in the output representation.\n",
        "        Equivalently, number of units in this layer.\n",
        "    do_activation: bool\n",
        "        Whether or not to apply ReLU activation.\n",
        "    postfix: string\n",
        "        Postfix on name scopes in this layer.\n",
        "        Used to simplify visualizations.\n",
        "    name: string\n",
        "        Name of layer.\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    A tensor representing the activations of this layer.\n",
        "    '''\n",
        "    def __init__(self, dim_input, dim_output, do_activation=True, postfix='', name=None):\n",
        "        super().__init__(name=name)\n",
        "        with tf.name_scope('dense' + postfix):\n",
        "            self.do_activation = do_activation\n",
        "            self.weights = tf.Variable(tf.initializers.he_uniform()(shape=(dim_input,dim_output), dtype=tf.float32), \\\n",
        "                                   name='weights'+postfix)\n",
        "            self.bias = tf.Variable(tf.zeros_initializer()(shape=(dim_output), dtype=tf.float32), name='bias'+postfix)\n",
        "      \n",
        "        \n",
        "    def __call__(self, x):\n",
        "        if self.do_activation:\n",
        "            return tf.nn.relu(tf.math.add(tf.matmul(x, self.weights), self.bias))\n",
        "        else:\n",
        "            return tf.math.add(tf.matmul(x, self.weights), self.bias)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wzKXc2k1KRe"
      },
      "source": [
        "class Conv(tf.Module):\n",
        "    '''\n",
        "    Creates a convolutional layer module.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    input_channels: int\n",
        "        Number of channels in the input representation.\n",
        "    n_filters: int\n",
        "        Number of channels in the output representation.\n",
        "        Equivalently, number of filters in this layer.\n",
        "    filter_size: int\n",
        "        Width and height of each kernel in the layer's filters.\n",
        "    stride: int\n",
        "        Stride to use in the x and y directions for the\n",
        "        convolution operation.\n",
        "    do_activation: bool\n",
        "        Whether or not to apply ReLU activation.\n",
        "    pool_size: int\n",
        "        If > 1, does max pooling of this size to the\n",
        "        width and height axes of the activation.\n",
        "    postfix: string\n",
        "        Postfix on name and variable scopes in this layer.\n",
        "        Used to simplify visualizations.\n",
        "    name: string\n",
        "        Name of layer\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    A tensor representing the activations of this layer.\n",
        "    '''\n",
        "    def __init__(self, input_channels, n_filters, \n",
        "                 filter_size=3, stride=1,\n",
        "                 do_activation=True, pool_size=1,\n",
        "                 postfix='', name=None):\n",
        "        super().__init__(name=name)\n",
        "        with tf.name_scope('conv' + postfix):\n",
        "            self.do_activation = do_activation\n",
        "            self.pool_size = pool_size\n",
        "            self.stride = stride\n",
        "            self.filters = tf.Variable(tf.initializers.he_uniform()\\\n",
        "                                       (shape= (filter_size,filter_size,input_channels,n_filters), \\\n",
        "                                        dtype=tf.float32), name='filters'+postfix)\n",
        "            self.bias = tf.Variable(tf.zeros_initializer()\\\n",
        "                                       (shape= (n_filters), \\\n",
        "                                        dtype=tf.float32), name='bias'+postfix)\n",
        "            \n",
        "            \n",
        "    def image(self, x):\n",
        "      conv = tf.nn.conv2d(x, self.filters, strides = [1, self.stride, self.stride, 1], padding='SAME')\n",
        "      relu = tf.nn.bias_add(conv, self.bias)\n",
        "      if self.do_activation:\n",
        "          relu = tf.nn.relu(relu)\n",
        "      return relu\n",
        "            \n",
        "    def __call__(self, x):\n",
        "        toReturn = self.image(x)\n",
        "        if self.pool_size > 1:\n",
        "            toReturn = tf.nn.max_pool(toReturn, ksize = [1, self.pool_size, self.pool_size, 1], strides=[1, self.pool_size, self.pool_size, 1], padding='VALID')\n",
        "        return toReturn"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na3Tt4YCrhIb"
      },
      "source": [
        "### 3.2: The rest of the model\n",
        "Write a `tf.Module` class called `Surrogate` that uses your classes from last week to be your surrogate model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i9QNP9_rhIb"
      },
      "source": [
        "#### 3.2.1: Adversarial noise\n",
        "This is the only part of the model that really differs from last week, and it's a bit tricky.\n",
        "\n",
        "Add a variable to the graph that represents the adversarial noise we'll add to one example.\n",
        "It should be the shape of a single input image and initialized to zeros.\n",
        "In addition, pass (to the `Variable()` constructor) the keyword argument `constraint=lambda x: tf.clip_by_value(x, -0.3, 0.3)`.\n",
        "Every time the variable is updated, it will become the result of the lambda function, which in this case constrains its pixel values to be in the range -0.3 to 0.3.\n",
        "(Equivalently, at every step we re-project the adversarial noise back into a hypercube at the origin with side length 0.6).\n",
        "I found that 0.3 works well, but feel free to change this value -- smaller values will produce less obvious noise, while larger values will produce more successful attacks. \n",
        "\n",
        "Add a boolean `tf.Variable` (default False) named \"use_noise\" to the graph.\n",
        "This will act as a \"switch\" controlling whether a given run uses adversarial noise. Additionally, add the argument `trainable=False` to the constructor, as it makes no sense to update this variable during training.\n",
        "\n",
        "Add a `tf.cond()` operation that, depending on `use_noise`, switches between the input image and the image plus the adversarial noise.\n",
        "This allows us to train the surrogate model without adversarial noise, then enable it when crafting the adversarial example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGCvjHfWrhIb"
      },
      "source": [
        "class Surrogate(tf.Module):\n",
        "    def __init__(self, name=None):\n",
        "        self.adv_noise = tf.Variable(tf.zeros_initializer()(shape= (1,64,64,1), \\\n",
        "              dtype=tf.float32),constraint=lambda x: tf.clip_by_value(x, -0.3, 0.3))\n",
        "        self.use_noise = tf.Variable(False, trainable=False)\n",
        "        self.conv_layer1 = Conv(1, 16, filter_size=5, stride=2, do_activation=True, pool_size=1, postfix='Conv Layer 1')\n",
        "        self.conv_layer2 = Conv(16, 16, filter_size=3, stride=1, do_activation=True, pool_size=2, postfix='Conv Layer 2')\n",
        "        self.conv_layer3 = Conv(16, 32, filter_size=3, stride=1, do_activation=True, pool_size=1, postfix='Conv Layer 3')\n",
        "        self.conv_layer4 = Conv(32, 32, filter_size=3, stride=1, do_activation=True, pool_size=2, postfix='Conv Layer 4')\n",
        "        self.conv_layer5 = Conv(32, 64, filter_size=3, stride=1, do_activation=True, pool_size=1, postfix='Conv Layer 5')\n",
        "        self.conv_layer6 = Conv(64, 64, filter_size=3, stride=1, do_activation=True, pool_size=2, postfix='Conv Layer 6')\n",
        "        self.dense_layer7 = Dense(16*64, 128, do_activation=True, postfix='Dense Layer 7') \n",
        "        self.dense_layer8 = Dense(128, 10, do_activation=False, postfix='Dense Layer 8')\n",
        "        \n",
        "    def logits(self, x):\n",
        "        conv_all = self.conv_layer6(self.conv_layer5(self.conv_layer4(self.conv_layer3(self.conv_layer2(self.conv_layer1(x))))))\n",
        "        conv_all = tf.reshape(conv_all, shape = [-1, 16*64])\n",
        "        dense_all = self.dense_layer8(self.dense_layer7(conv_all))\n",
        "        return tf.squeeze(dense_all)\n",
        "    \n",
        "    @tf.function\n",
        "    def __call__(self, x):\n",
        "        add = tf.cond(self.use_noise, lambda: x + self.adv_noise, lambda: x)\n",
        "        logits = self.logits(add)\n",
        "        return tf.nn.softmax(logits)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3kpCMmkrhIc"
      },
      "source": [
        "\n",
        "\n",
        "### 3.3: Compute cross-entropy loss\n",
        "Write a `_loss` function to compute the cross-entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh-qqIJmrhIc"
      },
      "source": [
        "def _loss(target, actual):\n",
        "    mse_per_example = tf.nn.softmax_cross_entropy_with_logits(target, actual)\n",
        "    mse_batch = tf.reduce_mean(mse_per_example)  \n",
        "    return mse_batch"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkMJv4hXrhIc"
      },
      "source": [
        "### 3.4: Surrogate and adversarial loss and gradients\n",
        "Add an optimizer and train method we'll use to train the surrogate model to act like the target model.\n",
        "It should just minimize the cross-entropy loss between the model's predictions and the targets. When `use_noise` is false, we will not have any gradients to update the adversarial noise tensor with because it will not be referenced in our computation. This will cause TensorFlow to throw (very annoying) warning messages about nonexistent gradients. To suppress this, we can use this neat trick (credit [here](https://stackoverflow.com/questions/60022462/how-to-suppress-specific-warning-in-tensorflow-python)) to preprocess our gradients,\n",
        "```\n",
        "gradients = g.gradient(loss, model.trainable_variables)\n",
        "surrogate_optimizer.apply_gradients( \n",
        "                            (grad, var) for (grad, var) in zip(gradients, model.trainable_variables)\n",
        "                             if grad is not None)\n",
        "```\n",
        "\n",
        "When training the surrogate model, we want to change the model parameters to minimize the loss.\n",
        "When creating the adversarial example, we want to do something very different: _change the adversarial noise tensor alone to maximize the loss_.\n",
        "\n",
        "So, add an `adversarial=False` argument to your train method so we can toggle whether or not to train the surrogate model, or the adversarial noise.\n",
        "\n",
        "Define a new adversarial loss which is the negative of the original loss, and a new optimizer to minimize it.\n",
        "When you call `minimize()`, pass in the keyword argument `var_list=[model.noise]`, where \"noise\" is the name of your adversarial noise tensor.\n",
        "This will prevent the optimizer from changing the weights of the model when we optimize the adversarial noise.\n",
        "\n",
        "Add a summary scalar to plot the adversarial loss decreasing. Then, add a summary histogram and a summary image for the adversarial noise tensor, so we can plot it as we're learning the adversarial example.\n",
        "Finally, add a summary image for the output of `cond`, which will be our adversarial example later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRO4HsDvrhId"
      },
      "source": [
        "surrogate_optimizer = tf.optimizers.SGD(1e-3, momentum=0.9)\n",
        "adversarial_optimizer = tf.optimizers.SGD(1e-3, momentum=0.9)\n",
        "\n",
        "# finds the accuracy\n",
        "def accurate(target, actual):\n",
        "    ti = tf.argmax(target, axis=1)\n",
        "    ai = tf.argmax(actual, axis=1)\n",
        "    toReturn = tf.reduce_mean(tf.cast(tf.math.equal(ti, ai) ,tf.float32))\n",
        "    return toReturn\n",
        "\n",
        "def layer_operations(curr_idx, x, layers):\n",
        "    if curr_idx > 0:\n",
        "        x = layer_operations(curr_idx-1, x, layers)\n",
        "    return layers[curr_idx](x)\n",
        "\n",
        "def train(model, x, y, i, adversarial=False):\n",
        "    optimizer = surrogate_optimizer\n",
        "    var_list = model.trainable_variables\n",
        "    writer = surrogate_writer\n",
        "\n",
        "    if adversarial:\n",
        "        optimizer = adversarial_optimizer\n",
        "        var_list = [model.adv_noise]\n",
        "        writer = adversarial_writer\n",
        "\n",
        "    with tf.GradientTape() as g:\n",
        "        loss = _loss(y, model.logits(x))\n",
        "    accuracy = accurate(y, model(x))\n",
        "    if model.use_noise:\n",
        "        optimizer.minimize(-loss, var_list)\n",
        "    else:\n",
        "        gradients = g.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients((grad, var) for (grad, var) in \\\n",
        "                                zip(gradients, model.trainable_variables) \\\n",
        "                                if grad is not None) \n",
        "    with writer.as_default():\n",
        "        tf.summary.scalar('loss', loss, step=i)\n",
        "        tf.summary.scalar('accuracy', accuracy, step=i)\n",
        "        tf.summary.histogram('adversarial_noise', model.adv_noise, step=i)\n",
        "        #tf.summary.image(\"osmething\", , step=i)\n",
        "    return accuracy"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3yv3klHrhId"
      },
      "source": [
        "## Section 4: Train the surrogate model\n",
        "Train your surrogate model until it hits high accuracy.\n",
        "At the very least it should have 90% accuracy -- I hit 98-100% on the training set.\n",
        "Overfitting isn't really a concern here since we are actually trying to memorize the target model.\n",
        "\n",
        "When the training loop is done, save the model under `./checkpoints/model_surrogate`.\n",
        "We won't be modifying this particular checkpoint any more, it'll contain the fully-trained surrogate model with zero adversarial noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "EQzzDJq9rhId",
        "outputId": "bce939d7-9332-4c3d-852c-5de529d3ee1d"
      },
      "source": [
        "model = Surrogate()\n",
        "train_batch = 0\n",
        "\n",
        "# Training loop\n",
        "for i in range(n_epochs):\n",
        "    test_accuracy = []\n",
        "    for x, y in dataset:  \n",
        "        if train_batch == 0:\n",
        "            tf.summary.trace_on(graph=True, profiler=True)\n",
        "            model(x)\n",
        "            with surrogate_writer.as_default():\n",
        "                tf.summary.trace_export(name='first training batch', step=0, profiler_outdir='./logs/surrogate')\n",
        "        test_accuracy.append(train(model, x, y, train_batch))\n",
        "        train_batch += 1\n",
        "\n",
        "    print('Epoch:\\t', i)\n",
        "    print('Average Training Set Accuracy:\\t', np.mean(test_accuracy))\n",
        "    \n",
        "# Save model\n",
        "checkpoint = tf.train.Checkpoint(model=model)\n",
        "checkpoint.write('./checkpoints/model_surrogate')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 64, 64, 1)\n",
            "(14, 64, 64, 1)\n",
            "Epoch:\t 0\n",
            "Average Training Set Accuracy:\t 0.5354853\n",
            "Epoch:\t 1\n",
            "Average Training Set Accuracy:\t 0.8940247\n",
            "Epoch:\t 2\n",
            "Average Training Set Accuracy:\t 0.95796704\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./checkpoints/model_surrogate'"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7P93YpmrhIf"
      },
      "source": [
        "## Section 5: Learn the adversarial example\n",
        "Finally, it's time to create an adversarial example using our surrogate model.\n",
        "Pick an image from the test set (or use the one I picked below), then run a training loop to minimize the adversarial loss (equivalently, maximize the model loss on that example).\n",
        "To do this, use `surrogate_model.use_noise.assign(True)`.\n",
        "This turns on the `use_noise` switch we set before, and overwrites the input image with the image we're going to turn into an adversarial example.\n",
        "\n",
        "When it's done, save it to a new checkpoint, `./checkpoints/model_adversarial`.\n",
        "This new checkpoint should have exactly the same model parameters as before (remember that we're only optimizing the noise), but a nonzero noise tensor.\n",
        "\n",
        "At the end, also save the adversarial example (which will be contained in the result of your `cond`) and the noise tensor to a numpy array so we can use them later.\n",
        "\n",
        "Periodically write the image and histogram summaries so you can look at them in TensorBoard later. \n",
        "\n",
        "You should be able to do this just by setting `adversarial=True` in your train method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "BWysmf5xrhIh",
        "outputId": "ce968fc2-8039-4739-873e-6cb29df09489"
      },
      "source": [
        "# Pick and plot the image we'll turn into an adversarial example\n",
        "idx = 1\n",
        "img = x_test[idx]\n",
        "lbl = y_test[idx]\n",
        "\n",
        "plt.imshow(img[:, :, 0], cmap='gray')\n",
        "plt.title('True label: {}'.format(np.argmax(lbl)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'True label: 9')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "FAjODO2prhIi",
        "outputId": "f07f8583-02e7-4a55-9ba7-86960b190d12"
      },
      "source": [
        "model.use_noise.assign(True)\n",
        "train_batch = 0\n",
        "\n",
        "# Training loop\n",
        "for i in range(n_epochs):\n",
        "    test_accuracy = []\n",
        "    for x, y in dataset:  \n",
        "        if train_batch == 0:\n",
        "            tf.summary.trace_on(graph=True, profiler=True)\n",
        "            model(x)\n",
        "            with adversarial_writer.as_default():\n",
        "                tf.summary.trace_export(name='first training batch', step=0, profiler_outdir='./logs/adversarial')\n",
        "        if train_batch % 5 == 0:\n",
        "          test_accuracy.append(train(model, x, y, train_batch, adversarial=True))\n",
        "        else:\n",
        "          test_accuracy.append(train(model, x, y, train_batch))\n",
        "        train_batch += 1\n",
        "\n",
        "    print('Epoch:\\t', i)\n",
        "    print('Average Training Set Accuracy:\\t', np.mean(test_accuracy))\n",
        "    \n",
        "# Save model\n",
        "checkpoint = tf.train.Checkpoint(model=model)\n",
        "checkpoint.write('./checkpoints/model_adversarial')\n",
        "\n",
        "##adv_example = np.array(model.image_to_use(x)) needs editing\n",
        "adv_noise = np.array(model.adv_noise)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-618f3a90a93d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'first training batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler_outdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./logs/adversarial'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain_batch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m           \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madversarial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m           \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-525c5d156009>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, x, y, i, adversarial)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccurate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_noise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \"\"\"\n\u001b[1;32m    530\u001b[0m     grads_and_vars = self._compute_gradients(\n\u001b[0;32m--> 531\u001b[0;31m         loss, var_list=var_list, grad_loss=grad_loss, tape=tape)\n\u001b[0m\u001b[1;32m    532\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss, tape)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0;31m# TODO(joshl): Test that we handle weight decay in a reasonable way.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m       raise ValueError(\"`tape` is required when a `Tensor` loss is passed. \"\n\u001b[0m\u001b[1;32m    567\u001b[0m                        f\"Received: loss={loss}, tape={tape}.\")\n\u001b[1;32m    568\u001b[0m     \u001b[0mtape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: `tape` is required when a `Tensor` loss is passed. Received: loss=-0.0777902603149414, tape=None."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7Bs6g49rhIi"
      },
      "source": [
        "## Section 6: Visualize the adversarial example\n",
        "Plot the original example, the adversarial example, and the adversarial noise below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDpXy15krhIj"
      },
      "source": [
        "plt.imshow(img[:, :, 0], cmap='gray')\n",
        "plt.title('True label: {}'.format(np.argmax(lbl)))\n",
        "\n",
        "#plt.imshow(adv_example[:,:,0], cmap='gray')\n",
        "plt.imshow(adv_noise[:,:,0], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2haNG2j_rhIj"
      },
      "source": [
        "## Section 7: Demonstrate that the surrogate model fails on the adversarial example\n",
        "Compute the surrogate model's prediction and probability for the original example and the adversarial example, and compare them. Remember to set the `use_noise` switch to false when computing predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwrFeVt_rhIj"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqrVFcXJrhIk"
      },
      "source": [
        "## 8: Demonstrate that the target model fails on the adversarial example\n",
        "This is the real test -- transferring the learned adversarial example from the surrogate model to the target model.\n",
        "Compute the target model's prediction and probability for the original example and the adversarial example, and compare them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF3DseRurhIk"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_P_rlLxrhIk"
      },
      "source": [
        "# Congratulations, you've carried out a black-box adversarial attack!\n",
        "These kinds of adversarial attacks are a serious concern in the real world, since they can be [made to work when printed on paper](https://blog.openai.com/robust-adversarial-inputs/) and [we're still bad at defending against them](https://blog.openai.com/adversarial-example-research/).\n",
        "\n",
        "Some interesting things to try:\n",
        " - Did the surrogate model and the target model misclassify the adversarial example in the same way? (Mine did)\n",
        " - How subtle can you make the noise while still tricking the target network?\n",
        " - Look at how the adversarial example evolves through training in TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlDZabLFuebU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}