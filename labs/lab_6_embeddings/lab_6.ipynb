{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "lab_6.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDZT0BtAc2jN"
      },
      "source": [
        "# Lab 6: Word2Vec from scratch in TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u00iTc3rc2ja"
      },
      "source": [
        "In this lab, we'll write Word2Vec, a particularly popular, simple, and powerful model for unsupervised learning of embedding vectors for words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFVZ34cziBHw"
      },
      "source": [
        "# Link : https://colab.research.google.com/drive/1s3Xr4yIPbZvl94bmIc49LcLpGYrvPHda#scrollTo=AFVZ34cziBHw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FP4Ng_mc2jb"
      },
      "source": [
        "## Understanding Word2Vec\n",
        "Word2Vec trains embeddings by using them as input to a logistic regression model, trained to predict a word given nearby words.\n",
        "\n",
        "Alternatively, you can think of Word2Vec as a neural network with linear activations and one hidden layer, trained to predict a word given nearby words.\n",
        "In this interpretation, the input and output of the model are both one-hot encoded vectors, and the embeddings of the words are the activations of the hidden layer given that word's one-hot encoding as input.\n",
        "Since the inputs are one-hot encoded, **it's more efficient (but equivalent) to use an embedding lookup instead of a matrix multiply for the first layer.**\n",
        "\n",
        "The data is produced by taking any large body of text (e.g. Wikipedia) and creating (context, target) pairs, where the target is any word and the context is the $n$ words to its left and right.\n",
        "\n",
        "There are two common ways of training Word2Vec:\n",
        " - The \"skip-gram\" model uses the target word as input and predicts context words.\n",
        " - The \"continuous bag-of-words\" (CBOW) model uses the context words as input and predicts the target word\n",
        " \n",
        "We'll focus on the CBOW model, since it tends to work better for small datasets.\n",
        "\n",
        "The process of generating the data for CBOW is as follows:\n",
        " 1. Tokenize (convert words, which are strings, to integer \"tokens\" indicating the word) the dataset by assigning each word a unique integer (integer encoding)\n",
        " 2. For each word in the dataset, create a single (`context`, `target`) pair, where `target` is the integer encoding of the word and `context` is a list of the integer encodings of the $n=1$ words to the left and right of the target word\n",
        "\n",
        "(I've already written the code to do this for you below)\n",
        " \n",
        "Then, the model is trained to predict the one-hot encoding of the target word given the integer encodings of the context words:\n",
        " 1. For each context word, look up its embedding in a table\n",
        " 2. Combine these into an \"average context embedding,\" which is the depth-wise average of the embeddings of the individual context words. This results in a single embedding vector, which acts as the \"total context\" in some sense.\n",
        " 3. Perform a logistic regression **(equivalently, a single dense layer with softmax activation)** to predict the target word using the average context embedding as input.\n",
        "\n",
        "Instead of full logistic regression, which uses the softmax function over all of the many words that appear in the dataset, **we will use candidate sampling (specifically, noise-contrastive estimation) to compute the loss.**\n",
        "This should speed up training significantly.\n",
        "\n",
        "For more info on Word2Vec, see [TensorFlow's \"Vector Representations of Words\" tutorial](https://www.tensorflow.org/tutorials/representation/word2vec), or Alex Minnaar's two tutorials, [one on the skip-gram model](http://alexminnaar.com/2015/04/12/word2vec-tutorial-skipgram.html) and [the other on the CBOW model](http://alexminnaar.com/2015/05/18/word2vec-tutorial-continuousbow.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0g9tf26c2jc"
      },
      "source": [
        "## Section 0: Download and preprocess the data\n",
        "The dataset is the Cornell Movie-Dialogs Corpus, a collection of dialogue from movie scripts.\n",
        "Download it from the link [here](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html), then unzip it in a subfolder called \"data\".\n",
        "Your directory tree should include the file `./data/cornell movie-dialogs corpus/movie_lines.txt`.\n",
        "\n",
        "I've written all the code for loading and preprocessing the data below, but read through it to understand what's going on.\n",
        "You'll need to use parts of it later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4h7WcsSc2je"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from scipy.spatial import distance"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "qn-K20FHhzTS",
        "outputId": "fa14e911-e096-4287-c81c-27aba8efe88f"
      },
      "source": [
        "# I used this to upload the data files onto the google colab platform\n",
        "# I also had to manually create a data and cornell movie-dialogs corpus folder\n",
        "\n",
        "from google.colab import files\n",
        "def getLocalFiles():\n",
        "    _files = files.upload()\n",
        "    if len(_files) >0:\n",
        "       for k,v in _files.items():\n",
        "         open(k,'wb').write(v)\n",
        "getLocalFiles()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-72cca0d1-d7a9-468b-a925-2354f13537c8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-72cca0d1-d7a9-468b-a925-2354f13537c8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KivKZSb9c2jh"
      },
      "source": [
        "### 0.1: Read the data\n",
        "First, we read the lines as separate strings from the text file.\n",
        "\n",
        "If you are running into memory problems (i.e. running out of RAM while working on this assignment) you can increase `skip_header` to train on a smaller dataset (but your resulting embeddings will be less good).\n",
        "There are about 300,000 lines total, so you can set `skip_header` to skip some percentage of those."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUXqxJuWc2ji"
      },
      "source": [
        "# Ignore warnings on incorrectly-formatted inputs\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "_, _, _, _, lines = np.genfromtxt(\n",
        "    './data/cornell movie-dialogs corpus/movie_lines.txt',\n",
        "    dtype='<U128', \n",
        "    delimiter='+++$+++', autostrip=True,\n",
        "    encoding='latin1', invalid_raise=False, unpack=True,\n",
        "    skip_header=0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aczwDbMlc2jk"
      },
      "source": [
        "### 0.2: Tokenize\n",
        "Keras has a built-in tokenization utility, which we'll use.\n",
        "This creates two dictionaries, which you'll need to use later:\n",
        " - `tokenizer.word_index` maps from string words to integer tokens\n",
        " - `tokenizer.index_word` maps from integer tokens to string words\n",
        "\n",
        "Then, we convert the dialogue lines into lists of tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kPIV3Gsc2jl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cb53546-8c07-4820-a09d-ee0e32d4f6b2"
      },
      "source": [
        "# Create a tokenizer and assign each word an integer\n",
        "tokenizer = keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "\n",
        "# Convert the lines to lists of integers\n",
        "tokenized_lines = tokenizer.texts_to_sequences(lines)\n",
        "\n",
        "# Delete the original lines to save memory\n",
        "del(lines)\n",
        "\n",
        "# Here's an example of how to use word_index and index_word\n",
        "print('The integer encoding of \"yes\" is:',\n",
        "      tokenizer.word_index['yes'])\n",
        "print('The word with integer encoding 10 is:',\n",
        "      tokenizer.index_word[10])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The integer encoding of \"yes\" is: 67\n",
            "The word with integer encoding 10 is: what\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OATC3up6c2jn"
      },
      "source": [
        "### 0.3: Create (context, target) pairs\n",
        "In this case, \"target\" means the integer encoding of a word, and \"context\" means a list of the integer encodings of the words to its left and right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgqmBVbec2jo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c5dbdc6-aca4-49b2-cd5e-4a036a786d1f"
      },
      "source": [
        "window_size = 1  # Consider 1 word on each side of target\n",
        "stride = 2       # Avoid window overlap\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "# Add (context, target) pairs to the dataset\n",
        "for line in tokenized_lines:\n",
        "    # Do not use lines that are too short\n",
        "    if len(line) < 2 * window_size + 1:\n",
        "        continue\n",
        "    \n",
        "    for target_idx in range(window_size, \n",
        "                            len(line) - window_size, \n",
        "                            stride):\n",
        "        target = line[target_idx]\n",
        "        left_context = line[target_idx - window_size : \n",
        "                            target_idx]\n",
        "        right_context = line[target_idx + 1 :\n",
        "                             target_idx + window_size + 1]\n",
        "        \n",
        "        Y.append(target)\n",
        "        X.append(left_context + right_context)\n",
        "\n",
        "# Convert to ndarrays\n",
        "X = np.asarray(X)\n",
        "Y = np.asarray(Y)\n",
        "\n",
        "# These constants may be useful for you later\n",
        "n_words = max(tokenizer.word_index.values())\n",
        "n_examples = len(X)\n",
        "\n",
        "print('n_words:', n_words)\n",
        "print('n_examples:', n_examples)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_words: 54034\n",
            "n_examples: 1178349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNO2oPwAc2jo"
      },
      "source": [
        "### 0.4: Make a TSV metadata file\n",
        "This creates a metadata file in the format that the TensorBoard Projector uses (tab-separated values).\n",
        "This will let us see the names for words later when we visualize the embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpqVyVGzc2jp"
      },
      "source": [
        "import os\n",
        "\n",
        "# Make a logs directory if none exists yet\n",
        "os.makedirs('./logs', exist_ok=True)\n",
        "\n",
        "# Make TSV metadata file\n",
        "with open('./logs/metadata.tsv', 'w') as f:\n",
        "    # Header specifies column name\n",
        "    f.write('Word\\tIndex\\tCount\\n')\n",
        "    \n",
        "    # Unrecognized values have an integer encoding of 0\n",
        "    f.write('UNK\\t0\\t0\\n')\n",
        "    \n",
        "    # One word per line\n",
        "    for i in range(1, n_words):\n",
        "        word = tokenizer.index_word[i]\n",
        "        count = tokenizer.word_counts[word]\n",
        "        f.write('{}\\t{}\\t{}\\n'.format(word, count, i))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2-nwOyUc2jp"
      },
      "source": [
        "### 0.5: Build a TensorFlow data pipeline\n",
        "I've set up the `tf.data.Dataset` and the `tf.summary.SummaryWriter` for you.\n",
        "Feel free to change the training hyperparameters and transforms if you like.\n",
        "\n",
        "You might want to try:\n",
        " - Changing the batch size to suit your RAM / GPU memory situation\n",
        " - Removing `.cache()` from the transforms if you can't fit the whole dataset in RAM\n",
        " - Changing n_negative_samples. Increasing it makes computing the loss slower but more accurate per batch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzT_9wkac2jq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "976aef3c-d4f2-4d47-f333-46312df47188"
      },
      "source": [
        "# Training hyperparameters\n",
        "n_epochs = 10\n",
        "batch_size = 64\n",
        "n_batches_per_epoch = n_examples / batch_size\n",
        "print('Batches per epoch:', n_batches_per_epoch)\n",
        "\n",
        "# Number of wrong words to sample when computing\n",
        "# the loss using noise-contrastive estimation.\n",
        "n_negative_samples = 128\n",
        "\n",
        "# Construct dataset and apply transforms\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X, Y)) \\\n",
        "    .shuffle(1000) \\\n",
        "    .batch(batch_size) \\\n",
        "    .cache() \\\n",
        "    .repeat(n_epochs)\n",
        "\n",
        "writer = tf.summary.create_file_writer('./logs')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches per epoch: 18411.703125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMRTejt1c2jq"
      },
      "source": [
        "## Section 1: Build a model graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awK6yKOLc2jr"
      },
      "source": [
        "### 1.0: Model hyperparameters\n",
        "`embedding_size` is the number of dimensions in the embedding vector of each word.\n",
        "Try to train 64-dimensional embeddings, but if training takes too long, feel free to reduce this to 32 or 16.\n",
        "\n",
        "`validation_words` is a list of words we'll use to see how good our embeddings are.\n",
        "While training, we'll periodically print out the words that have embeddings most similar to these words.\n",
        "As training progresses, this should start printing words with similar meanings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUw1z-7qc2jr"
      },
      "source": [
        "embedding_size = 64\n",
        "validation_words = ['yes', 'small', 'thousand']"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLqy1gEnc2js"
      },
      "source": [
        "### 1.1: Word2Vec class\n",
        "\n",
        "Write a `tf.Module` class called `Word2Vec`. We are concerned with a tensor which holds the context integer encodings (shape: `(batch_size, 2)`) and a tensor which holds the target integer encoding (shape: `(batch_size,)`) which will be our x and y variables respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSHi2Nq1c2js"
      },
      "source": [
        "#### 1.1.1: Embedding variable\n",
        "Make a variable which holds the embeddings for each of the words.\n",
        "It should be a rank-2 tensor (a matrix) where the $i$-th row is the embedding vector for the word with integer encoding $i$.\n",
        "Its shape should be `(n_words, embedding_size)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WU0SzxPc2jt"
      },
      "source": [
        "#### 1.1.2: Create the variables for logistic regression\n",
        "Create weight and bias variables for a logistic regression which takes in the average context embedding and outputs a probability for each word.\n",
        "\n",
        "NOTE: We won't actually ever compute the output of this logistic regression by hand (i.e. the matrix multiplication and softmax activation), since we're using noise-contrastive estimation to approximate it.\n",
        "TensorFlow's `nce_loss` function just takes the weights and bias tensors as input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdSmXNw8c2jt"
      },
      "source": [
        "#### 1.1.3: Compute the average context embedding\n",
        "Look up the embeddings of each of the context words (with a single call to `tf.nn.embedding_lookup`), then average them depth-wise into a single average embedding vector with shape `(batch_size, embedding_size)`. Do this in a method called `context_average_embedding` that takes in a `context` argument."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dty3WOodc2jt"
      },
      "source": [
        "#### 1.1.4: Compute the loss\n",
        "We want to jointly train the logistic regression weights and the word embeddings using cross-entropy loss on the output of the logistic regression, but this is inefficient because of the large number of outputs (we'd need one logit for each word).\n",
        "\n",
        "Instead, approximate the per-example loss using noise-contrastive estimation by calling `tf.nn.nce_loss`, then return the mean loss for the batch.\n",
        "\n",
        "Since we will require many variables from our model to compute this loss, do this in a method called `loss` in your model class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHIIeo-8c2ju"
      },
      "source": [
        "#### 1.1.5: Operations find similar embeddings\n",
        "The **cosine similarity** between two embeddings $\\vec{x}$ and $\\vec{y}$ is the cosine of the angle between them, computed as:\n",
        "$$\n",
        "\\cos(\\vec{x}, \\vec{y}) = \\frac{\\vec{x} \\cdot \\vec{y}}{||\\vec{x}|| \\cdot ||\\vec{y}||}\n",
        "$$\n",
        "This is a more robust similarity measure than Euclidean distance, since it's invariant to scaling the embedding vectors by a constant.\n",
        "\n",
        "We'll use cosine similarity to find the most similar words to any given input word.\n",
        "The steps for doing this are:\n",
        " 1. Take a word's integer encoding as input, and compute its embedding with `tf.nn.embedding_lookup`.\n",
        " 2. Compute a \"similarity tensor\" which holds cosine similarity of this embedding with all of the word embeddings. This should result in a single tensor with shape `(n_words,).\n",
        " 3. Use `tf.nn.top_k` to find the top 8 similarities and their indices in the similarity tensor. These indices will be the integer encodings of the words with the most similar embeddings to the input. Return this tensor.\n",
        "\n",
        "Later, you can find the most similar words to a given input word by running the tensor that holds the top 8 indices, then using `tokenizer.index_word` on each of those indices.\n",
        "\n",
        "When debugging this, make sure that the most similar word to a given word is that word itself, with a cosine similarity of 1.\n",
        "\n",
        "Do this under a method called `compute_similar_words`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4YCrjkRc2ju"
      },
      "source": [
        "class Word2Vec(tf.Module):\n",
        "    '''\n",
        "    Creates a Word2Vec module.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    n_words: integer\n",
        "        Number of words.\n",
        "\n",
        "    embedding_size: integer\n",
        "        Size of the embeddings.\n",
        "\n",
        "    postfix: string\n",
        "        Postfix\n",
        "\n",
        "    name: string\n",
        "        Name of layer.\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    A tensor representing the activations of this layer.\n",
        "    '''\n",
        "    def __init__(self, n_words=n_words, embedding_size=embedding_size, postfix='', name=None):\n",
        "        super().__init__(name=name)\n",
        "        with tf.name_scope('Word2Vec' + postfix):\n",
        "            # Input shape = [batch_size, 2]\n",
        "            # Weight shape = [n_words, 2]\n",
        "            # Bias shape = [n_words]\n",
        "            self.embeddings = tf.Variable(tf.initializers.he_uniform()\\\n",
        "                              (shape=(n_words, embedding_size), dtype=tf.float32), \\\n",
        "                              name='embeddings' + postfix)\n",
        "            self.n_words = n_words\n",
        "            self.weights = tf.Variable(tf.initializers.he_uniform()(shape=(batch_size, 2), dtype=tf.float32), \\\n",
        "                                   name='weights'+postfix)\n",
        "            self.bias = tf.Variable(tf.zeros_initializer()(shape=(batch_size,), dtype=tf.float32), name='bias'+postfix)\n",
        "      \n",
        "    def find_embedding(self):\n",
        "        return lambda word_ids: tf.nn.embedding_lookup(self.embeddings, word_ids,\n",
        "                                                      name = 'embeddings')\n",
        "  \n",
        "    def context_average_embedding(self, context):\n",
        "        embeddings = self.find_embedding()(context)\n",
        "        em_vec = tf.reduce_mean(embeddings(context), axis=1)\n",
        "        print(em_vec.shape)\n",
        "        return em_vec\n",
        "    \n",
        "    @tf.function()\n",
        "    def loss(self, target, actual):\n",
        "        print('actual: ',actual.shape)\n",
        "        print('target: ',target.shape)\n",
        "        print('weights: ',self.weights.shape)\n",
        "        print('bias: ',self.bias.shape)\n",
        "        print('expanded target: ',tf.expand_dims(target,axis=1).shape)\n",
        "        print(actual)\n",
        "        actual = tf.reshape(self.find_embedding()(actual),[64,2*64])\n",
        "        print(actual.shape)\n",
        "        target = tf.reshape(self.find_embedding()(target), [64,64])\n",
        "        print(target.shape)\n",
        "        loss = tf.nn.nce_loss(weights=self.weights, biases=self.bias, labels=target, inputs=actual,\\\n",
        "                              num_sampled=n_negative_samples, num_classes=n_words)\n",
        "        return tf.reduce_mean(loss)\n",
        "    \n",
        "    def compute_similar_words(self, word):\n",
        "        orig_embedding = self.find_embedding(word)\n",
        "        sim_all = tf.constant(0, shape=(self.n_words,), dtype=tf.float32, name='similarity')\n",
        "        for i in range(n_words):\n",
        "          curr_embedding = self.find_embedding(i)\n",
        "          sim_all[i] = 1 - distance.cosine(orig_embedding, curr_embedding)\n",
        "        print(sim_all.shape)\n",
        "        top8 = tf.nn.top_k(sim_all, 8)\n",
        "        return top8\n",
        "\n",
        "    def __call__(self, x, y):\n",
        "        #embeddings = self.context_average_embedding(x)\n",
        "        loss = self.loss(y, self.find_embedding()(x))\n",
        "        return loss\n",
        "\n",
        "        "
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x2o9Gljc2jv"
      },
      "source": [
        "### 1.2: Optimizer and gradients\n",
        "Make an optimizer (I used `tf.optimizers.Adam` with `learning_rate=1e-3`) and a `train` method with a `bool` switch to compute summaries. If this switch is true, add a summary scalar plot to plot the loss in TensorBoard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xtXNk4hc2jv"
      },
      "source": [
        "optimizer = tf.optimizers.Adam(learning_rate=1e-3)\n",
        "train_batch = 0\n",
        "\n",
        "def train(model, x, y, i, summaries=False):\n",
        "    with tf.GradientTape() as g:\n",
        "        loss = _model.loss(x, y)\n",
        "    #gradients = g.gradient(loss, model.trainable_variables)\n",
        "    optimizer.minimize(loss, model.trainable_variables)\n",
        "    # optimizer.apply_gradients((grad, var) for (grad, var) in \\\n",
        "                            #      zip(gradients, model.trainable_variables) \\\n",
        "                             #     if grad is not None) \n",
        "    if summaries:\n",
        "        with writer.as_default():\n",
        "            tf.summary.scalar('loss', loss, step=i)     \n",
        "\n"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAPOTzOXc2jw"
      },
      "source": [
        "## Section 2: Train the model\n",
        "Use the same kind of training loop we've used before, repeatedly calling the method that applies gradient updates.\n",
        "\n",
        "The dataset is large, but the model doesn't do much work for each example.\n",
        "So, you might want to only run and save the summaries every 1000 batches or so to speed up training.\n",
        "\n",
        "In addition, every 1000 batches, for every word in `validation_words`, print the 8 most similar words by cosine similarity.\n",
        "\n",
        "Finally, use a `tf.train.Checkpoint()` to save the model's weights in `./logs`, which is necessary to visualize the embeddings in TensorBoard. Since we only care about the weights, initialize the checkpoint as `tf.train.Checkpoint(embedding=model.word_embeddings)`. Instead of `checkpoint.write(...)`, use `checkpoint.save('./logs/embedding.ckpt')` for a format TensorBoard can interpret in its embedding projector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y8J3jYzc2jw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "outputId": "f2a49490-1854-4832-f52a-f042e708f404"
      },
      "source": [
        "model = Word2Vec()\n",
        "train_batch = 0\n",
        "\n",
        "idx = 11\n",
        "\n",
        "# Training loop\n",
        "for i in range(n_epochs):\n",
        "    # Iterate over dataset once\n",
        "    for x, y in dataset:  \n",
        "        print(x.shape)\n",
        "        print(y.shape)\n",
        "        summaries = False\n",
        "        if train_batch == 0:\n",
        "            # On the first batch, run a full trace\n",
        "            tf.summary.trace_on(graph=True, profiler=True)\n",
        "            # We simply run this operation to add our graph to TensorBoard\n",
        "            model.loss(y, x)\n",
        "            with writer.as_default():\n",
        "                tf.summary.trace_export(name='first training batch', step=0, profiler_outdir='./logs')\n",
        "        # Call train iteration\n",
        "        if train_batch & 100 == 0:\n",
        "            summaries = True\n",
        "        train(model, x, y, train_batch, summaries)\n",
        "        if train_batch % 1000 == 0:\n",
        "          print(model.top8(validation_words))\n",
        "        train_batch += 1\n",
        "\n",
        "    print('Epoch:\\t', i)\n",
        "    \n",
        "# Save model\n",
        "checkpoint = tf.train.Checkpoint(embedding=model.word_embeddings)\n",
        "checkpoint.save('./logs/embedding.ckpt') "
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 2)\n",
            "(64,)\n",
            "WARNING:tensorflow:Trace already enabled\n",
            "actual:  (64, 2)\n",
            "target:  (64,)\n",
            "weights:  (64, 2)\n",
            "bias:  (64,)\n",
            "expanded target:  (64, 1)\n",
            "Tensor(\"actual:0\", shape=(64, 2), dtype=int64)\n",
            "(64, 128)\n",
            "(64, 64)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-21695c2cf3b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_on\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# We simply run this operation to add our graph to TensorBoard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'first training batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler_outdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./logs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"<ipython-input-90-ecec4edff137>\", line 57, in loss  *\n        loss = tf.nn.nce_loss(weights=self.weights, biases=self.bias, labels=target, inputs=actual,                              num_sampled=n_negative_samples, num_classes=n_words)\n\n    ValueError: Dimensions must be equal, but are 128 and 2 for '{{node nce_loss/MatMul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=true](Reshape, nce_loss/Slice_1)' with input shapes: [64,128], [128,2].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFkvZ00pc2jx"
      },
      "source": [
        "## Section 3: Visualize the learned embeddings\n",
        "Run TensorBoard pointed at `./logs` and look in the Projector tab, then use the \"Load data\" button and select the `./logs/metadata.tsv` file we created earlier to load word labels. If projector does not show up, use the URL [`http://localhost:6006#projector`](http://localhost:6006#projector) depending on the port TensorBoard is using (mine uses 6006).\n",
        "\n",
        "Try typing some words into the search bar and see which words come up as most similar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkjnHN_8c2jx"
      },
      "source": [
        "## Section 4 (optional): Generate analogies\n",
        "Try using embedding vector arithmetic to generate analogies.\n",
        "To do this:\n",
        " - Compute the embedding vector for several vectors using `tf.nn.embedding_lookup`\n",
        " - Do vector arithmetic on the computed embeddings\n",
        " - Find the most similar word embeddings by cosine similarity, then find the words that map to those embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRhjIZe0c2jy"
      },
      "source": [
        "# Your code here?"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}